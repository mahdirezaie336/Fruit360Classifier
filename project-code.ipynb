{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading training set features\n",
    "f = open(\"Datasets/train_set_features.pkl\", \"rb\")\n",
    "train_set_features2 = pickle.load(f)\n",
    "f.close()\n",
    "\n",
    "# reducing feature vector length \n",
    "features_STDs = np.std(a=train_set_features2, axis=0)\n",
    "train_set_features = train_set_features2[:, features_STDs > 52.3]\n",
    "\n",
    "# changing the range of data between 0 and 1\n",
    "train_set_features = np.divide(train_set_features, train_set_features.max())\n",
    "\n",
    "# loading training set labels\n",
    "f = open(\"Datasets/train_set_labels.pkl\", \"rb\")\n",
    "train_set_labels = pickle.load(f)\n",
    "f.close()\n",
    "\n",
    "# ------------\n",
    "# loading test set features\n",
    "f = open(\"Datasets/test_set_features.pkl\", \"rb\")\n",
    "test_set_features2 = pickle.load(f)\n",
    "f.close()\n",
    "\n",
    "# reducing feature vector length \n",
    "features_STDs = np.std(a=test_set_features2, axis=0)\n",
    "test_set_features = test_set_features2[:, features_STDs > 48]\n",
    "\n",
    "# changing the range of data between 0 and 1\n",
    "test_set_features = np.divide(test_set_features, test_set_features.max())\n",
    "\n",
    "# loading test set labels\n",
    "f = open(\"Datasets/test_set_labels.pkl\", \"rb\")\n",
    "test_set_labels = pickle.load(f)\n",
    "f.close()\n",
    "\n",
    "# ------------\n",
    "# preparing our training and test sets - joining datasets and lables\n",
    "train_set = []\n",
    "test_set = []\n",
    "\n",
    "for i in range(len(train_set_features)):\n",
    "    label = np.array([0,0,0,0])\n",
    "    label[int(train_set_labels[i])] = 1\n",
    "    label = label.reshape(4,1)\n",
    "    train_set.append((train_set_features[i].reshape(102,1), label))\n",
    "    \n",
    "\n",
    "for i in range(len(test_set_features)):\n",
    "    label = np.array([0,0,0,0])\n",
    "    label[int(test_set_labels[i])] = 1\n",
    "    label = label.reshape(4,1)\n",
    "    test_set.append((test_set_features[i].reshape(102,1), label))\n",
    "\n",
    "# shuffle\n",
    "random.shuffle(train_set)\n",
    "random.shuffle(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1/(1 + np.exp(-z, dtype=np.longdouble))\n",
    "\n",
    "def d_sigmoid(z):\n",
    "    return sigmoid(z) * (1 - sigmoid(z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'axis' is an invalid keyword argument for sum()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_11176/1161793097.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_11176/1161793097.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumn_stack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtest_set\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m     \u001b[0mmlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Accuracy:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_11176/1161793097.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, x, y, batch_size, epoch_number)\u001b[0m\n\u001b[1;32m     45\u001b[0m                 \u001b[0mw\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearning_rate\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m                 \u001b[0mb\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearning_rate\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me0\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m                 \u001b[0mb\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearning_rate\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m                 \u001b[0mb\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearning_rate\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'axis' is an invalid keyword argument for sum()"
     ]
    }
   ],
   "source": [
    "class MLP:\n",
    "    \n",
    "    weights: list[np.ndarray]\n",
    "    biases: list[np.ndarray]\n",
    "    \n",
    "    def __init__(self, layers_size: list[tuple[int, int]]):\n",
    "        self.weights = []\n",
    "        self.biases = []\n",
    "        self.learning_rate = 1\n",
    "        \n",
    "        for size in layers_size:\n",
    "            self.weights.append(np.random.normal(size=size))\n",
    "            self.biases.append(np.zeros((1, size[1])).astype(np.longdouble))\n",
    "        \n",
    "        self.layers_number = len(self.weights)\n",
    "\n",
    "    def feed(self, data):\n",
    "        W = self.weights\n",
    "        b = self.biases\n",
    "        \n",
    "        result = []\n",
    "        o = sigmoid(W[0].T.dot(data) + b[0].T)\n",
    "        result.append(o)\n",
    "        for i in range(1, self.layers_number):\n",
    "            o = sigmoid(W[i].T.dot(o) + b[i].T)\n",
    "            result.append(o)\n",
    "        return result\n",
    "    \n",
    "    def train(self, x, y, batch_size=10, epoch_number=10):\n",
    "        w = self.weights\n",
    "        b = self.biases\n",
    "        for i in range(epoch_number):\n",
    "            for j in range(0, x.shape[1], batch_size):\n",
    "                data = x[:, j:j+batch_size]       # A batch of inputs\n",
    "                y_ex = y[:, j:j+batch_size]       # Expected Output\n",
    "                out = self.feed(data)             # Outputs of each layer for the given batch\n",
    "                \n",
    "                # \n",
    "                e2 = (out[2] - y_ex) * d_sigmoid(out[2])\n",
    "                e1 = w[2].dot(e2) * d_sigmoid(out[1])\n",
    "                e0 = w[1].dot(e1) * d_sigmoid(out[0])\n",
    "                \n",
    "                w[0] -= self.learning_rate * (1/batch_size) * data.dot(e0.T)\n",
    "                w[1] -= self.learning_rate * (1/batch_size) * out[0].dot(e1.T)\n",
    "                w[2] -= self.learning_rate * (1/batch_size) * out[1].dot(e2.T)\n",
    "                \n",
    "                b[0] -= self.learning_rate * (1/batch_size) * sum(e0.T, axis=0)\n",
    "                b[1] -= self.learning_rate * (1/batch_size) * sum(e1.T, axis=0)\n",
    "                b[2] -= self.learning_rate * (1/batch_size) * sum(e2.T, axis=0)\n",
    "                \n",
    "    def test(self, x, y):\n",
    "        out = self.feed(x)\n",
    "        maximum = np.argmax(out[-1], axis=0)\n",
    "        equality = (maximum == np.argmax(y)).astype('int')\n",
    "        return sum(equality) / y.shape[1]\n",
    "        \n",
    "\n",
    "def main():\n",
    "    # Create neural network object\n",
    "    mlp = MLP([(102, 150), (150, 60), (60, 4)])\n",
    "    \n",
    "    # Seperate x and y of train set\n",
    "    x_train = np.column_stack([i[0] for i in train_set])\n",
    "    y_train = np.column_stack([i[1] for i in train_set])\n",
    "    \n",
    "    # Seperate x and y of test set\n",
    "    x_test = np.column_stack([i[0] for i in test_set])\n",
    "    y_test = np.column_stack([i[1] for i in test_set])\n",
    "    \n",
    "    mlp.train(x_train, y_train)\n",
    "    print('Accuracy:', mlp.test(x_test, y_test))\n",
    "    \n",
    "    \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(102, 2)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.column_stack([train_set[0][0], train_set[1][0]]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 2])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(np.array([[1, 1, 1],\n",
    "                    [2, 1, 1],\n",
    "                    [1, 2, 3]]), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 0],\n",
       "       [1, 0, 0]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(np.array([[1, 1, 1],\n",
    "           [1, 1, 1]]) == np.array([[1, 1, 2],\n",
    "                                    [1, 0, 21]])).astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
