{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading training set features\n",
    "f = open(\"Datasets/train_set_features.pkl\", \"rb\")\n",
    "train_set_features2 = pickle.load(f)\n",
    "f.close()\n",
    "\n",
    "# reducing feature vector length \n",
    "features_STDs = np.std(a=train_set_features2, axis=0)\n",
    "train_set_features = train_set_features2[:, features_STDs > 52.3]\n",
    "\n",
    "# changing the range of data between 0 and 1\n",
    "train_set_features = np.divide(train_set_features, train_set_features.max())\n",
    "\n",
    "# loading training set labels\n",
    "f = open(\"Datasets/train_set_labels.pkl\", \"rb\")\n",
    "train_set_labels = pickle.load(f)\n",
    "f.close()\n",
    "\n",
    "# ------------\n",
    "# loading test set features\n",
    "f = open(\"Datasets/test_set_features.pkl\", \"rb\")\n",
    "test_set_features2 = pickle.load(f)\n",
    "f.close()\n",
    "\n",
    "# reducing feature vector length \n",
    "features_STDs = np.std(a=test_set_features2, axis=0)\n",
    "test_set_features = test_set_features2[:, features_STDs > 48]\n",
    "\n",
    "# changing the range of data between 0 and 1\n",
    "test_set_features = np.divide(test_set_features, test_set_features.max())\n",
    "\n",
    "# loading test set labels\n",
    "f = open(\"Datasets/test_set_labels.pkl\", \"rb\")\n",
    "test_set_labels = pickle.load(f)\n",
    "f.close()\n",
    "\n",
    "# ------------\n",
    "# preparing our training and test sets - joining datasets and lables\n",
    "train_set = []\n",
    "test_set = []\n",
    "\n",
    "for i in range(len(train_set_features)):\n",
    "    label = np.array([0,0,0,0])\n",
    "    label[int(train_set_labels[i])] = 1\n",
    "    label = label.reshape(4,1)\n",
    "    train_set.append((train_set_features[i].reshape(102,1), label))\n",
    "    \n",
    "\n",
    "for i in range(len(test_set_features)):\n",
    "    label = np.array([0,0,0,0])\n",
    "    label[int(test_set_labels[i])] = 1\n",
    "    label = label.reshape(4,1)\n",
    "    test_set.append((test_set_features[i].reshape(102,1), label))\n",
    "\n",
    "# shuffle\n",
    "random.shuffle(train_set)\n",
    "random.shuffle(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1/(1 + np.exp(-z, dtype=np.longdouble))\n",
    "\n",
    "def d_sigmoid(z):\n",
    "    return sigmoid(z) * (1 - sigmoid(z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "class MLP:\n",
    "    \n",
    "    weights: list[np.ndarray]\n",
    "    biases: list[np.ndarray]\n",
    "    \n",
    "    def __init__(self, layers_size: list[tuple[int, int]]):\n",
    "        self.weights = []\n",
    "        self.biases = []\n",
    "        self.learning_rate = 1\n",
    "        \n",
    "        for size in layers_size:\n",
    "            self.weights.append(np.random.normal(size=size))\n",
    "            self.biases.append(np.zeros((1, size[1])).astype(np.longdouble))\n",
    "        \n",
    "        self.layers_number = len(self.weights)\n",
    "\n",
    "    def feed(self, data):\n",
    "        W = self.weights\n",
    "        b = self.biases\n",
    "        \n",
    "        result = []\n",
    "        o = sigmoid(W[0].T.dot(data) + b[0].T)\n",
    "        result.append(o)\n",
    "        for i in range(1, self.layers_number):\n",
    "            o = sigmoid(W[i].T.dot(o) + b[i].T)\n",
    "            result.append(o)\n",
    "        return result\n",
    "    \n",
    "    def train(self, x, y, batch_size=10, epoch_number=5, validation=0.2):\n",
    "        w = self.weights\n",
    "        b = self.biases\n",
    "        \n",
    "        for i in range(epoch_number):\n",
    "            for j in range(0, x.shape[1], batch_size):\n",
    "                data = x[:, j:j+batch_size]       # A batch of inputs\n",
    "                y_ex = y[:, j:j+batch_size]       # Expected Output\n",
    "                out = self.feed(data)             # Outputs of each layer for the given batch\n",
    "                \n",
    "                # Back Propagation Phase\n",
    "                e2 = (out[2] - y_ex) * d_sigmoid(out[2])\n",
    "                e1 = w[2].dot(e2) * d_sigmoid(out[1])\n",
    "                e0 = w[1].dot(e1) * d_sigmoid(out[0])\n",
    "                \n",
    "                w[0] -= self.learning_rate * (1/batch_size) * data.dot(e0.T)\n",
    "                w[1] -= self.learning_rate * (1/batch_size) * out[0].dot(e1.T)\n",
    "                w[2] -= self.learning_rate * (1/batch_size) * out[1].dot(e2.T)\n",
    "                \n",
    "                b[0] -= self.learning_rate * (1/batch_size) * np.sum(e0.T, axis=0)\n",
    "                b[1] -= self.learning_rate * (1/batch_size) * np.sum(e1.T, axis=0)\n",
    "                b[2] -= self.learning_rate * (1/batch_size) * np.sum(e2.T, axis=0)\n",
    "                \n",
    "    def test(self, x, y):\n",
    "        out = self.feed(x)\n",
    "        maximum = np.argmax(out[-1], axis=0)\n",
    "        equality = (maximum == np.argmax(y, axis=0)).astype('int')\n",
    "        return sum(equality) / y.shape[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seperate x and y of train set\n",
    "x_train = np.column_stack([i[0] for i in train_set])\n",
    "y_train = np.column_stack([i[1] for i in train_set])\n",
    "\n",
    "# Seperate x and y of test set\n",
    "x_test = np.column_stack([i[0] for i in test_set])\n",
    "y_test = np.column_stack([i[1] for i in test_set])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train by 200 items\n",
    "\n",
    "Here we train our model by 200 items of train set and finally we will witness that it has accuracy around 0.95."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy before train: 0.21\n",
      "Accuracy after train: 0.9833836858006042\n"
     ]
    }
   ],
   "source": [
    "# Create neural network object\n",
    "mlp = MLP([(102, 150), (150, 60), (60, 4)])\n",
    "\n",
    "# Using 200 items of test set to see the accuracy for Phase 1\n",
    "print('Accuracy before train:', mlp.test(x_test[:, :200], y_test[:, :200]))\n",
    "\n",
    "# Training the model\n",
    "mlp.train(x_train[:, :200], y_train[:, :200])\n",
    "\n",
    "# Using whole test set to see the accuracy\n",
    "print('Accuracy after train:', mlp.test(x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and Test with whole datasets\n",
    "\n",
    "In this part first we make a numpy array of the train_set and test_set. After that we seperate 200 items of test set to see the accuray of model. It is close to 0.25 normally. Then we train an we will see the accuracy rises up to 1.0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create neural network object\n",
    "mlp = MLP([(102, 150), (150, 60), (60, 4)])\n",
    "\n",
    "# Using 200 items of test set to see the accuracy for Phase 1\n",
    "print('Accuracy before train:', mlp.test(x_test[:, :200], y_test[:, :200]))\n",
    "\n",
    "# Training the model\n",
    "mlp.train(x_train, y_train)\n",
    "\n",
    "# Using whole test set to see the accuracy\n",
    "print('Accuracy after train:', mlp.test(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'builtin_function_or_method' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_53293/2145004650.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m*\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: 'builtin_function_or_method' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "np.split(np.array[[1, 1, 1, 1, 1]], [0.2* 5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
