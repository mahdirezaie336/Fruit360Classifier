{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading training set features\n",
    "f = open(\"Datasets/train_set_features.pkl\", \"rb\")\n",
    "train_set_features2 = pickle.load(f)\n",
    "f.close()\n",
    "\n",
    "# reducing feature vector length \n",
    "features_STDs = np.std(a=train_set_features2, axis=0)\n",
    "train_set_features = train_set_features2[:, features_STDs > 52.3]\n",
    "\n",
    "# changing the range of data between 0 and 1\n",
    "train_set_features = np.divide(train_set_features, train_set_features.max())\n",
    "\n",
    "# loading training set labels\n",
    "f = open(\"Datasets/train_set_labels.pkl\", \"rb\")\n",
    "train_set_labels = pickle.load(f)\n",
    "f.close()\n",
    "\n",
    "# ------------\n",
    "# loading test set features\n",
    "f = open(\"Datasets/test_set_features.pkl\", \"rb\")\n",
    "test_set_features2 = pickle.load(f)\n",
    "f.close()\n",
    "\n",
    "# reducing feature vector length \n",
    "features_STDs = np.std(a=test_set_features2, axis=0)\n",
    "test_set_features = test_set_features2[:, features_STDs > 48]\n",
    "\n",
    "# changing the range of data between 0 and 1\n",
    "test_set_features = np.divide(test_set_features, test_set_features.max())\n",
    "\n",
    "# loading test set labels\n",
    "f = open(\"Datasets/test_set_labels.pkl\", \"rb\")\n",
    "test_set_labels = pickle.load(f)\n",
    "f.close()\n",
    "\n",
    "# ------------\n",
    "# preparing our training and test sets - joining datasets and lables\n",
    "train_set = []\n",
    "test_set = []\n",
    "\n",
    "for i in range(len(train_set_features)):\n",
    "    label = np.array([0,0,0,0])\n",
    "    label[int(train_set_labels[i])] = 1\n",
    "    label = label.reshape(4,1)\n",
    "    train_set.append((train_set_features[i].reshape(102,1), label))\n",
    "    \n",
    "\n",
    "for i in range(len(test_set_features)):\n",
    "    label = np.array([0,0,0,0])\n",
    "    label[int(test_set_labels[i])] = 1\n",
    "    label = label.reshape(4,1)\n",
    "    test_set.append((test_set_features[i].reshape(102,1), label))\n",
    "\n",
    "# shuffle\n",
    "random.shuffle(train_set)\n",
    "random.shuffle(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "W1 = np.random.rand(102, 150)\n",
    "W2 = np.random.rand(150, 60)\n",
    "W3 = np.random.rand(60, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[9.44670209e-01],\n",
       "        [3.60737731e-01],\n",
       "        [3.29790560e-01],\n",
       "        [2.42888403e-01],\n",
       "        [1.69115349e-01],\n",
       "        [8.34635824e-02],\n",
       "        [6.40825258e-02],\n",
       "        [6.50203188e-02],\n",
       "        [3.75117224e-02],\n",
       "        [2.40700219e-02],\n",
       "        [9.09659269e-02],\n",
       "        [5.31416068e-03],\n",
       "        [3.43857455e-02],\n",
       "        [3.12597687e-03],\n",
       "        [2.25070334e-02],\n",
       "        [2.18818381e-02],\n",
       "        [5.00156299e-03],\n",
       "        [1.28165052e-02],\n",
       "        [2.18818381e-03],\n",
       "        [5.62675836e-03],\n",
       "        [5.93935605e-03],\n",
       "        [1.78180681e-02],\n",
       "        [9.37793060e-04],\n",
       "        [1.87558612e-03],\n",
       "        [1.34417005e-02],\n",
       "        [3.43857455e-03],\n",
       "        [1.56298843e-03],\n",
       "        [1.87558612e-03],\n",
       "        [3.12597687e-04],\n",
       "        [9.06533292e-03],\n",
       "        [1.56298843e-03],\n",
       "        [4.06376993e-03],\n",
       "        [1.87558612e-03],\n",
       "        [2.18818381e-03],\n",
       "        [1.56298843e-03],\n",
       "        [2.81337918e-03],\n",
       "        [1.25039075e-03],\n",
       "        [6.25195374e-04],\n",
       "        [1.56298843e-03],\n",
       "        [2.50078149e-02],\n",
       "        [0.00000000e+00],\n",
       "        [9.37793060e-04],\n",
       "        [5.00156299e-02],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [7.18974680e-03],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [1.21913098e-02],\n",
       "        [0.00000000e+00],\n",
       "        [9.37793060e-04],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [6.25195374e-04],\n",
       "        [6.22069397e-02],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [3.65739294e-02],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [1.29102845e-01]]),\n",
       " array([[1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0]]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b1 = np.zeros((150, 1))\n",
    "b2 = np.zeros((60, 1))\n",
    "b3 = np.zeros((4, 1))\n",
    "b3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1/(1 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagate(data):\n",
    "    o1 = sigmoid(np.matmul(W1.T, data) + b1)\n",
    "    o2 = sigmoid(np.matmul(W2.T, o1) + b2)\n",
    "    return sigmoid(np.matmul(W3.T, o2) + b3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = 0\n",
    "random.shuffle(train_set)\n",
    "for training_data in train_set[:200]:\n",
    "    data, flag = training_data\n",
    "    out = forward_propagate(data)\n",
    "    if np.argmax(out) == np.argmax(flag):\n",
    "        counter += 1\n",
    "counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stage 1) Random starting synaptic weights: \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_33025/1287808647.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0mtraining_set_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_set_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_set\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m     \u001b[0mneural_network\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_set_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_set_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;31m#     print (\"Stage 2) New synaptic weights after training: \")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_33025/1287808647.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, training_set_inputs, training_set_outputs, number_of_training_iterations)\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[0mlayer1_delta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer1_error\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__sigmoid_derivative\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_from_layer_1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m             \u001b[0mlayer1_adjustment\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtraining_set_inputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer1_delta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m             \u001b[0mlayer2_adjustment\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput_from_layer_1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer2_delta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m             \u001b[0mlayer3_adjustment\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput_from_layer_2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer3_delta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from numpy import exp, array, random, dot\n",
    "\n",
    "\n",
    "class NeuronLayer():\n",
    "    def __init__(self, number_of_neurons, number_of_inputs_per_neuron):\n",
    "        self.synaptic_weights = 2 * random.random((number_of_inputs_per_neuron, number_of_neurons)) - 1\n",
    "\n",
    "\n",
    "class NeuralNetwork():\n",
    "    def __init__(self, layer1, layer2, layer3):\n",
    "        self.layer1 = layer1\n",
    "        self.layer2 = layer2\n",
    "        self.layer3 = layer3\n",
    "\n",
    "    def __sigmoid(self, x):\n",
    "        return 1 / (1 + exp(-x))\n",
    "\n",
    "    def __sigmoid_derivative(self, x):\n",
    "        return x * (1 - x)\n",
    "\n",
    "    def train(self, training_set_inputs, training_set_outputs, number_of_training_iterations):\n",
    "        for iteration in range(number_of_training_iterations):\n",
    "\n",
    "            output_from_layer_1, output_from_layer_2, output_from_layer_3 = self.think(training_set_inputs)\n",
    "\n",
    "            layer3_error = output_from_layer_3 - training_set_outputs\n",
    "            layer3_delta = layer3_error * self.__sigmoid_derivative(output_from_layer_3)\n",
    "\n",
    "            layer2_error = layer3_delta.dot(self.layer3.synaptic_weights.T)\n",
    "            layer2_delta = layer2_error * self.__sigmoid_derivative(output_from_layer_2)\n",
    "            \n",
    "            layer1_error = layer2_delta.dot(self.layer2.synaptic_weights.T)\n",
    "            layer1_delta = layer1_error * self.__sigmoid_derivative(output_from_layer_1)\n",
    "\n",
    "            layer1_adjustment = training_set_inputs.T.dot(layer1_delta)\n",
    "            layer2_adjustment = output_from_layer_1.T.dot(layer2_delta)\n",
    "            layer3_adjustment = output_from_layer_2.T.dot(layer3_delta)\n",
    "\n",
    "            self.layer1.synaptic_weights -= layer1_adjustment\n",
    "            self.layer2.synaptic_weights -= layer2_adjustment\n",
    "            self.layer3.synaptic_weights -= layer3_adjustment\n",
    "\n",
    "    def think(self, inputs):\n",
    "        output_from_layer1 = self.__sigmoid(dot(inputs, self.layer1.synaptic_weights))\n",
    "        output_from_layer2 = self.__sigmoid(dot(output_from_layer1, self.layer2.synaptic_weights))\n",
    "        output_from_layer3 = self.__sigmoid(dot(output_from_layer2, self.layer3.synaptic_weights))\n",
    "        return output_from_layer1, output_from_layer2, output_from_layer3\n",
    "\n",
    "    def print_weights(self):\n",
    "        print (\"    Layer 1 (4 neurons, each with 3 inputs): \")\n",
    "        print (self.layer1.synaptic_weights)\n",
    "        print (\"    Layer 2 (1 neuron, with 4 inputs):\")\n",
    "        print (self.layer2.synaptic_weights)\n",
    "        print (\"    Layer 3 (1 neuron, with 4 inputs):\")\n",
    "        print (self.layer3.synaptic_weights)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    random.seed(1)\n",
    "    layer1 = NeuronLayer(150, 102)\n",
    "    layer2 = NeuronLayer(60, 150)\n",
    "    layer3 = NeuronLayer(4, 60)\n",
    "\n",
    "    neural_network = NeuralNetwork(layer1, layer2, layer3)\n",
    "\n",
    "    print (\"Stage 1) Random starting synaptic weights: \")\n",
    "    #neural_network.print_weights()\n",
    "\n",
    "    training_set_inputs = train_set[0][0].T\n",
    "    training_set_outputs = train_set[0][1].T\n",
    "    \n",
    "    for i in range(1, 200):\n",
    "        training_set_inputs = np.append(training_set_inputs, train_set[i][0].T, axis=0) \n",
    "        training_set_outputs = np.append(training_set_outputs, train_set[i][1].T, axis=0) \n",
    "\n",
    "    neural_network.train(training_set_inputs, training_set_outputs, 10000)\n",
    "\n",
    "#     print (\"Stage 2) New synaptic weights after training: \")\n",
    "#     neural_network.print_weights()\n",
    "\n",
    "#     # Test the neural network with a new situation.\n",
    "#     print (\"Stage 3) Considering a new situation [1, 1, 0] -> ?: \")\n",
    "    hidden_state_1, hidden_state_2, output = neural_network.think(test_set[3][0].T)\n",
    "    print(np.argmax(output))\n",
    "    print(test_set[3][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.34038567, 0.9875439 , 0.06241618, 0.10542293, 0.99540598,\n",
       "        0.70705242, 0.66207744, 0.01787842, 0.63587001, 0.81886307],\n",
       "       [0.49838557, 0.52528179, 0.70868279, 0.35583025, 0.569955  ,\n",
       "        0.04482455, 0.00245255, 0.95159187, 0.08664972, 0.60093887],\n",
       "       [0.57805549, 0.03010427, 0.03366104, 0.2456181 , 0.82871919,\n",
       "        0.08941554, 0.00253807, 0.27358648, 0.11034523, 0.43640293],\n",
       "       [0.96810181, 0.67022438, 0.74611862, 0.53110544, 0.66997768,\n",
       "        0.05332429, 0.13893763, 0.18570835, 0.8537528 , 0.28026278],\n",
       "       [0.55259639, 0.87074235, 0.76328459, 0.66530263, 0.41040896,\n",
       "        0.84829124, 0.24793538, 0.49744908, 0.41812542, 0.38048768],\n",
       "       [0.18060839, 0.47882044, 0.64745763, 0.70944283, 0.16645018,\n",
       "        0.79622631, 0.90218773, 0.80213073, 0.96144065, 0.26406338],\n",
       "       [0.58476812, 0.14816434, 0.84676178, 0.16965609, 0.50954895,\n",
       "        0.21604452, 0.45202608, 0.9066893 , 0.74549836, 0.14987941],\n",
       "       [0.51951721, 0.57411097, 0.03407019, 0.42255501, 0.53457886,\n",
       "        0.52038969, 0.26493742, 0.39985595, 0.08432746, 0.20565631],\n",
       "       [0.90231392, 0.14336832, 0.43937575, 0.68242443, 0.70532711,\n",
       "        0.09545233, 0.36519457, 0.02123895, 0.36926096, 0.44661171],\n",
       "       [0.65624198, 0.7587557 , 0.63144824, 0.81971011, 0.90488595,\n",
       "        0.80355352, 0.9430928 , 0.59816827, 0.66500003, 0.29710538]])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N = 10\n",
    "a = np.random.rand(N,N)\n",
    "b = np.zeros((N,N+1))\n",
    "b[:,:-1] = a\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import exp, array, random, dot\n",
    "\n",
    "learning_rate = 1\n",
    "\n",
    "class NeuronLayer():\n",
    "    def __init__(self, number_of_neurons, number_of_inputs_per_neuron):\n",
    "        self.synaptic_weights =  random.normal(size=(number_of_inputs_per_neuron, number_of_neurons))\n",
    "\n",
    "\n",
    "def addCol(value, x):\n",
    "    if value == 1:\n",
    "        temp = np.ones((x.shape[0], 1))\n",
    "    if value == 0:\n",
    "        temp = np.zeros((x.shape[0], 1))\n",
    "    return np.hstack((temp, x))\n",
    "\n",
    "\n",
    "class NeuralNetwork():\n",
    "    def __init__(self, layer1, layer2, layer3):\n",
    "        self.layer1 = layer1\n",
    "        self.layer2 = layer2\n",
    "        self.layer3 = layer3\n",
    "\n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 + exp(-x))\n",
    "\n",
    "    def sigmoid_derivative(self, x):\n",
    "        return x * (1 - x)\n",
    "\n",
    "    def train(self, all_training_set_inputs, all_training_set_outputs, epochs):\n",
    "\n",
    "        for iteration in range(epochs):\n",
    "            for batch in range(300):\n",
    "              training_set_inputs =  all_training_set_inputs[batch * 10 : (batch + 1) * 10 - 1, :]\n",
    "              training_set_outputs = all_training_set_outputs[batch * 10 : (batch + 1) * 10 - 1, :]\n",
    "\n",
    "              output_from_layer_1, output_from_layer_2, output_from_layer_3 = self.think(training_set_inputs)\n",
    "\n",
    "              layer3_error = output_from_layer_3 - training_set_outputs\n",
    "              layer3_delta = layer3_error * self.sigmoid_derivative(output_from_layer_3)\n",
    "            \n",
    "              tempW3 = np.delete(self.layer3.synaptic_weights, 0, 0)\n",
    "              layer2_error = layer3_delta.dot(tempW3.T)\n",
    "              layer2_delta = layer2_error * self.sigmoid_derivative(output_from_layer_2)\n",
    "            \n",
    "              tempW2 = np.delete(self.layer2.synaptic_weights, 0, 0)\n",
    "              layer1_error = layer2_delta.dot(tempW2.T)\n",
    "              layer1_delta = layer1_error * self.sigmoid_derivative(output_from_layer_1)\n",
    "\n",
    "              layer1_adjustment = (1/10)*(addCol(1, training_set_inputs)).T.dot(layer1_delta)\n",
    "              layer2_adjustment = (1/10)*(addCol(1, output_from_layer_1)).T.dot(layer2_delta)\n",
    "              layer3_adjustment = (1/10)*(addCol(1, output_from_layer_2)).T.dot(layer3_delta)\n",
    "\n",
    "              self.layer1.synaptic_weights -= learning_rate * layer1_adjustment\n",
    "              self.layer2.synaptic_weights -= learning_rate * layer2_adjustment\n",
    "              self.layer3.synaptic_weights -= learning_rate * layer3_adjustment\n",
    "\n",
    "    def think(self, inputs):\n",
    "        output_from_layer1 = self.sigmoid(dot(addCol(1, inputs), self.layer1.synaptic_weights))\n",
    "        output_from_layer2 = self.sigmoid(dot(addCol(1, output_from_layer1), self.layer2.synaptic_weights))\n",
    "        output_from_layer3 = self.sigmoid(dot(addCol(1, output_from_layer2), self.layer3.synaptic_weights))\n",
    "        return output_from_layer1, output_from_layer2, output_from_layer3\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    random.seed(2)\n",
    "    layer1 = NeuronLayer(150, 103)\n",
    "    layer2 = NeuronLayer(60, 151)\n",
    "    layer3 = NeuronLayer(4, 61)\n",
    "\n",
    "    neural_network = NeuralNetwork(layer1, layer2, layer3)\n",
    "\n",
    "    training_set_inputs = train_set[0][0].T\n",
    "    training_set_outputs = train_set[0][1].T\n",
    "    \n",
    "    for i in range(1, 1400):\n",
    "        training_set_inputs = np.append(training_set_inputs, train_set[i][0].T, axis=0) \n",
    "        training_set_outputs = np.append(training_set_outputs, train_set[i][1].T, axis=0) \n",
    "\n",
    "    ans1 = 0\n",
    "    for test in test_set:\n",
    "      hidden_state_1, hidden_state_2, output = neural_network.think(test[0].T)\n",
    "      if np.argmax(output) == np.argmax(test[1]):\n",
    "        ans1 += 1\n",
    "\n",
    "    print(\"Before learn(Test):\", ans1/len(test_set) * 100)\n",
    "\n",
    "    ans2 = 0\n",
    "    for test in train_set[:1400]:\n",
    "      hidden_state_1, hidden_state_2, output = neural_network.think(test[0].T)\n",
    "      if np.argmax(output) == np.argmax(test[1]):\n",
    "        ans2 += 1\n",
    "\n",
    "    print(\"Before learn(Train):\", (ans2/1400) * 100)\n",
    "\n",
    "    neural_network.train(training_set_inputs, training_set_outputs, 10)\n",
    "\n",
    "    ans3 = 0\n",
    "    for test in test_set:\n",
    "      hidden_state_1, hidden_state_2, output = neural_network.think(test[0].T)\n",
    "      if np.argmax(output) == np.argmax(test[1]):\n",
    "        ans3 += 1\n",
    "\n",
    "    print(\"After learn(Test):\", ans3/len(test_set) * 100)\n",
    "\n",
    "    ans4 = 0\n",
    "    for test in train_set[:1400]:\n",
    "      hidden_state_1, hidden_state_2, output = neural_network.think(test[0].T)\n",
    "      if np.argmax(output) == np.argmax(test[1]):\n",
    "        ans4 += 1\n",
    "\n",
    "    print(\"After learn(Train):\", (ans4/1400) * 100)\n",
    "\n",
    "    # hidden_state_1, hidden_state_2, output = neural_network.think(test_set[35][0].T)\n",
    "    # print(np.argmax(output), output)\n",
    "    # print(test_set[35][1])\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
